{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-py3-none-any.whl size=11074433 sha256=8d6d0b62475a894b004ba04779d0333f4f2c43769bcd47e72d016cc921dd8422\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1c94cgb9/wheels/57/3c/44/bac9773a437855dc01e9bf30fc2064610227be5e94a84cd680\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.5\n",
      "    Uninstalling en-core-web-sm-2.2.5:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.5\n",
      "Successfully installed en-core-web-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class initializations\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold all input sentences\n",
    "sentences = []\n",
    "\n",
    "# Dictionary to hold sentences corresponding to respective discourse markers\n",
    "disc_sentences = {}\n",
    "\n",
    "# Remaining sentences which do not have discourse markers (To be used later to generate other kinds of questions)\n",
    "nondisc_sentences = []\n",
    "\n",
    "# List of auxiliary verbs\n",
    "aux_list = ['am', 'are', 'is', 'was', 'were', 'can', 'could', 'does', 'do', 'did', 'has', 'had', 'may', 'might', 'must',\n",
    "            'need', 'ought', 'shall', 'should', 'will', 'would']\n",
    "\n",
    "# List of all discourse markers\n",
    "discourse_markers = ['because', 'as a result', 'since', 'when', 'although', 'for example', 'for instance']\n",
    "\n",
    "# Different question types possible for each discourse marker\n",
    "qtype = {'because': ['Why'], 'since': ['When', 'Why'], 'when': ['When'], 'although': ['Yes/No'], 'as a result': ['Why'], \n",
    "        'for example': ['Give an example where'], 'for instance': ['Give an instance where'], 'to': ['Why']}\n",
    "\n",
    "# The argument which forms a question\n",
    "target_arg = {'because': 1, 'since': 1, 'when': 1, 'although': 1, 'as a result': 2, 'for example': 1, 'for instance': 1, \n",
    "              'to': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to tokenize and split into sentences\n",
    "def sentensify():\n",
    "    global sentences\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    fp = open('input.txt')\n",
    "    data = fp.read()\n",
    "    sentences = tokenizer.tokenize(data)\n",
    "    discourse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to generate the questions from sentences which have already been pre-processed.\n",
    "def generate_question(question_part, type):\n",
    "\n",
    "    ''' \n",
    "        question_part -> Part of input sentence which forms a question\n",
    "        type-> The type of question (why, where, etc)\n",
    "    '''\n",
    "    # Remove full stop and make first letter lower case\n",
    "    question_part = question_part[0].lower() + question_part[1:]\n",
    "    if(question_part[-1] == '.' or question_part[-1] == ','):\n",
    "        question_part = question_part[:-1]\n",
    "        \n",
    "    # Capitalizing 'i' since 'I' is recognized by parsers appropriately    \n",
    "    for i in range(0, len(question_part)):\n",
    "        if(question_part[i] == 'i'):\n",
    "            if((i == 0 and question_part[i+1] == ' ') or (question_part[i-1] == ' ' and question_part[i+1] == ' ')):\n",
    "                question_part = question_part[:i] + 'I' + question_part[i + 1: ]\n",
    "                \n",
    "    question = \"\"\n",
    "    if(type == 'Give an example where' or type == 'Give an instance where'):\n",
    "        question = type + \" \" + question_part + '?'\n",
    "        return question\n",
    "\n",
    "    aux_verb = False\n",
    "    res = None\n",
    "    \n",
    "    # Find out if auxiliary verb already exists\n",
    "    for i in range(len(aux_list)):\n",
    "        if(aux_list[i] in question_part.split()):\n",
    "            aux_verb = True\n",
    "            pos = i\n",
    "            break\n",
    "\n",
    "    # If auxiliary verb exists\n",
    "    if(aux_verb):\n",
    "        \n",
    "        # Tokeninze the part of the sentence from which the question has to be made\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        question_part = \"\"\n",
    "        fP = False\n",
    "        \n",
    "        for word, tag in tags:\n",
    "            if(word in ['I', 'We', 'we']):\n",
    "                question_part += 'you' + \" \"\n",
    "                fP = True\n",
    "                continue\n",
    "            question_part += word + \" \"\n",
    "\n",
    "        # Split across the auxiliary verb and prepend it at the start of question part\n",
    "        question = question_part.split(\" \" + aux_list[pos])\n",
    "        if(fP):\n",
    "             question = [\"were \"] + question\n",
    "        else:\n",
    "            question = [aux_list[pos] + \" \"] + question\n",
    "\n",
    "        # If Yes/No, no need to introduce question phrase\n",
    "        if(type == 'Yes/No'):\n",
    "            question += ['?']\n",
    "            \n",
    "        elif(type != \"non_disc\"):\n",
    "            question = [type + \" \"] + question + [\"?\"]\n",
    "            \n",
    "        else:\n",
    "            question = question + [\"?\"]\n",
    "         \n",
    "        question = ''.join(question)\n",
    "\n",
    "    # If auxilary verb does ot exist, it can only be some form of verb 'do'\n",
    "    else:\n",
    "        aux = None\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        comb = \"\"\n",
    "\n",
    "        '''There can be following combinations of nouns and verbs:\n",
    "            NN/NNP and VBZ  -> Does\n",
    "            NNS/NNPS(plural) and VBP -> Do\n",
    "            NN/NNP and VBN -> Did\n",
    "            NNS/NNPS(plural) and VBN -> Did\n",
    "        '''\n",
    "        \n",
    "        for tag in tags:\n",
    "            if(comb == \"\"):\n",
    "                if(tag[1] == 'NN' or tag[1] == 'NNP'):\n",
    "                    comb = 'NN'\n",
    "\n",
    "                elif(tag[1] == 'NNS' or tag[1] == 'NNPS'):\n",
    "                    comb = 'NNS'\n",
    "\n",
    "                elif(tag[1] == 'PRP'):\n",
    "                    if tag[0] in ['He','She','It']:\n",
    "                        comb = 'PRPS'\n",
    "                    else:\n",
    "                        comb = 'PRPP'\n",
    "                        tmp = question_part.split(\" \")\n",
    "                        tmp = tmp[1: ]\n",
    "                        if(tag[0] in ['I', 'we', 'We']):\n",
    "                            question_part = 'you ' + ' '.join(tmp)\n",
    "                            \n",
    "            if(res == None):\n",
    "                res = re.match(r\"VB*\", tag[1])\n",
    "                if(res):\n",
    "                    \n",
    "                    # Stem the verb\n",
    "                    question_part = question_part.replace(tag[0], stemmer.stem(tag[0]))\n",
    "                res = re.match(r\"VBN\", tag[1])\n",
    "                res = re.match(r\"VBD\", tag[1])\n",
    "\n",
    "        if(comb == 'NN'):\n",
    "            aux = 'does'\n",
    "            \n",
    "        elif(comb == 'NNS'):\n",
    "            aux = 'do'\n",
    "            \n",
    "        elif(comb == 'PRPS'):\n",
    "            aux = 'does'\n",
    "            \n",
    "        elif(comb == 'PRPP'):\n",
    "            aux = 'do'\n",
    "            \n",
    "        if(res and res.group() in ['VBD', 'VBN']):\n",
    "            aux = 'did'\n",
    "\n",
    "        if(aux):\n",
    "            if(type == \"non_disc\" or type == \"Yes/No\"):\n",
    "                question = aux + \" \" + question_part + \"?\"\n",
    "\n",
    "            else:\n",
    "                question = type + \" \" + aux + \" \" + question_part + \"?\"\n",
    "    if(question != \"\"):\n",
    "        question = question[0].upper() + question[1:]\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to get the named entities\n",
    "def get_named_entities(sent):\n",
    "    doc = nlp(sent)\n",
    "    named_entities = [(X.text, X.label_) for X in doc.ents]\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to get the required wh word\n",
    "def get_wh_word(entity, sent):\n",
    "    wh_word = \"\"\n",
    "    if entity[1] in ['TIME', 'DATE']:\n",
    "        wh_word = 'When'\n",
    "        \n",
    "    elif entity[1] == ['PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE']:\n",
    "        wh_word = 'What'\n",
    "        \n",
    "    elif entity[1] in ['PERSON']:\n",
    "            wh_word = 'Who'\n",
    "            \n",
    "    elif entity[1] in ['NORP', 'FAC' ,'ORG', 'GPE', 'LOC']:\n",
    "        index = sent.find(entity[0])\n",
    "        if index == 0:\n",
    "            wh_word = \"Who\"\n",
    "            \n",
    "        else:\n",
    "            wh_word = \"Where\"\n",
    "            \n",
    "    else:\n",
    "        wh_word = \"Where\"\n",
    "    return wh_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generate questions based on NER templates\n",
    "def generate_one_word_questions(sent):\n",
    "    \n",
    "    named_entities = get_named_entities(sent)\n",
    "    questions = []\n",
    "    \n",
    "    if not named_entities:\n",
    "        return questions\n",
    "    \n",
    "    for entity in named_entities:\n",
    "        wh_word = get_wh_word(entity, sent)\n",
    "        \n",
    "        if(sent[-1] == '.'):\n",
    "            sent = sent[:-1]\n",
    "        \n",
    "        if sent.find(entity[0]) == 0:\n",
    "            questions.append(sent.replace(entity[0],wh_word) + '?')\n",
    "            continue\n",
    "       \n",
    "        question = \"\"\n",
    "        aux_verb = False\n",
    "        res = None\n",
    "\n",
    "        for i in range(len(aux_list)):\n",
    "            if(aux_list[i] in sent.split()):\n",
    "                aux_verb = True\n",
    "                pos = i\n",
    "                break\n",
    "            \n",
    "        if not aux_verb:\n",
    "            pos = 9\n",
    "        \n",
    "        text = nltk.word_tokenize(sent)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        question_part = \"\"\n",
    "        \n",
    "        if wh_word == 'When':\n",
    "            word_list = sent.split(entity[0])[0].split()\n",
    "            if word_list[-1] in ['in', 'at', 'on']:\n",
    "                question_part = \" \".join(word_list[:-1])\n",
    "            else:\n",
    "                question_part = \" \".join(word_list)\n",
    "            \n",
    "            qp_text = nltk.word_tokenize(question_part)\n",
    "            qp_tags = nltk.pos_tag(qp_text)\n",
    "            \n",
    "            question_part = \"\"\n",
    "            \n",
    "            for i, grp in enumerate(qp_tags):\n",
    "                word = grp[0]\n",
    "                tag = grp[1]\n",
    "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
    "                    question_part += WordNetLemmatizer().lemmatize(word,'v') + \" \"\n",
    "                else:\n",
    "                    question_part += word + \" \"\n",
    "                \n",
    "            if question_part[-1] == ' ':\n",
    "                question_part = question_part[:-1]\n",
    "        \n",
    "        else:\n",
    "            for i, grp in enumerate(tags):\n",
    "                \n",
    "                #Break the sentence after the first non-auxiliary verb\n",
    "                word = grp[0]\n",
    "                tag = grp[1]\n",
    "\n",
    "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
    "                    question_part += word\n",
    "\n",
    "                    if i<len(tags) and 'NN' not in tags[i+1][1] and wh_word != 'When':\n",
    "                        question_part += \" \"+ tags[i+1][0]\n",
    "\n",
    "                    break\n",
    "                question_part += word + \" \"\n",
    "        question = question_part.split(\" \"+ aux_list[pos])\n",
    "        question = [aux_list[pos] + \" \"] + question\n",
    "        question = [wh_word+ \" \"] + question + [\"?\"]\n",
    "        question = ''.join(question)\n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to pre-process sentences which have discourse markers in them\n",
    "def discourse():\n",
    "    temp = []\n",
    "    target = \"\"\n",
    "    questions = []\n",
    "    global disc_sentences\n",
    "    disc_sentences = {}\n",
    "    for i in range(len(sentences)):\n",
    "        maxLen = 9999999\n",
    "        val = -1\n",
    "        for j in discourse_markers:\n",
    "            tmp = len(sentences[i].split(j)[0].split(' '))  \n",
    "            \n",
    "            # To get valid, first discourse marker.   \n",
    "            if(len(sentences[i].split(j)) > 1 and tmp >= 3 and tmp < maxLen):\n",
    "                maxLen = tmp\n",
    "                val = j\n",
    "                \n",
    "        if(val != -1):\n",
    "\n",
    "            # To initialize a list for every new key\n",
    "            if(disc_sentences.get(val, 'empty') == 'empty'):\n",
    "                disc_sentences[val] = []\n",
    "                \n",
    "            disc_sentences[val].append(sentences[i])\n",
    "            temp.append(sentences[i])\n",
    "\n",
    "\n",
    "    nondisc_sentences = list(set(sentences) - set(temp))\n",
    "    \n",
    "    t = []\n",
    "    for k, v in disc_sentences.items():\n",
    "        for val in range(len(v)):\n",
    "            \n",
    "            # Split the sentence on discourse marker and identify the question part\n",
    "            question_part = disc_sentences[k][val].split(k)[target_arg[k] - 1]\n",
    "            q = generate_question(question_part, qtype[k][0])\n",
    "            if(q != \"\"):\n",
    "                questions.append([disc_sentences[k][val],q])\n",
    "                \n",
    "                \n",
    "    for question_part in nondisc_sentences:\n",
    "        s = \"non_disc\"\n",
    "        sentence = question_part\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        if(text[0] == 'Yes'):\n",
    "            question_part = question_part[5:]\n",
    "            s = \"Yes/No\"\n",
    "            \n",
    "        elif(text[0] == 'No'):\n",
    "            question_part = question_part[4:]\n",
    "            s = \"Yes/No\"\n",
    "            \n",
    "        q = generate_question(question_part, s)\n",
    "        if(q != \"\"):\n",
    "            questions.append([sentence,q])\n",
    "        l = generate_one_word_questions(question_part)\n",
    "        questions += [[sentence,i] for i in l]\n",
    "    print(len(questions))\n",
    "    \n",
    "    for pair in questions:\n",
    "        print(\"S: \",pair[0])\n",
    "        print(\"Q: \",pair[1])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "0.7608695652173914\n",
      "0.8428571428571429\n"
     ]
    }
   ],
   "source": [
    "# Syntactic Score and Fluency using Manual Evaluation\n",
    "\n",
    "syntactic_score = [0,0,1,1,1,1,1,0,1,1,1,0,1,0,1,1,1,1,1,1,1,0,1,1,0,1,0,1,1,1,1,1,0,1,1,1,1,0,1,0,1,0,1,1,1,1,1,1,0,0,\n",
    "                    1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,0,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1,0,1,1,1,0,1,1]\n",
    "fluency_score   = [0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,0,1,0,1,1,1,1,1,0,0,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,\n",
    "                    1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,0,1,1,0,1,1,1,0,1,1,1,0,0,0,0,0,1,1]\n",
    "print(len(syntactic_score))\n",
    "print(sum(syntactic_score)/len(syntactic_score))\n",
    "print(sum(fluency_score)/sum(syntactic_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vyshak/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "S:  They were angry because their plans had been discovered.\n",
      "Q:  Why were they angry ?\n",
      "\n",
      "S:  I think he felt included because he was helping as much as we were.\n",
      "Q:  Why did you think he felt included ?\n",
      "\n",
      "S:  I am studying English because I’d like to immigrate to the U.S.\n",
      "Q:  Why were you studying English ?\n",
      "\n",
      "S:  Children often cry just because they want some attention.\n",
      "Q:  Why do children often cry just ?\n",
      "\n",
      "S:  Tom repainted his mailbox because it was looking shabby.\n",
      "Q:  Why did tom repaint his mailbox ?\n",
      "\n",
      "S:  They slept in the car because they couldn't find a hotel.\n",
      "Q:  Why do they slept in the car ?\n",
      "\n",
      "S:  The accident happened because of the driver's negligence.\n",
      "Q:  Why did the accident hap ?\n",
      "\n",
      "S:  He gave up traveling abroad because of his sudden illness.\n",
      "Q:  Why did he gav up traveling abroad ?\n",
      "\n",
      "S:  Harry was late for class yesterday because of his accident.\n",
      "Q:  Why was harry late for class yesterday ?\n",
      "\n",
      "S:  We had a bad rice crop last year because it rained a lot.\n",
      "Q:  Why were you a bad rice crop last year ?\n",
      "\n",
      "S:  I can play quite a few musical instruments, for example, the flute, the guitar, and the piano.\n",
      "Q:  Give an example where I can play quite a few musical instruments, ?\n",
      "\n",
      "S:  Calcium is found in green leafy vegetables, for example, broccoli, kale, arugula, or spinach have over 160 mg. per serving.\n",
      "Q:  Give an example where calcium is found in green leafy vegetables, ?\n",
      "\n",
      "S:  It is possible to combine Computer Science with other subjects, for example Physics.\n",
      "Q:  Give an example where it is possible to combine Computer Science with other subjects, ?\n",
      "\n",
      "S:  Fractions can be written with oblique strokes, for example 2/3.\n",
      "Q:  Give an example where fractions can be written with oblique strokes, ?\n",
      "\n",
      "S:  He hurt his hand when he fell.\n",
      "Q:  When did he hurt his hand ?\n",
      "\n",
      "S:  Every child feels displaced to some degree when a new sibling arrives.\n",
      "Q:  When does every child feels displac to some degree ?\n",
      "\n",
      "S:  She was angry when you told her about the accident.\n",
      "Q:  When was she angry ?\n",
      "\n",
      "S:  I married her when she was 23.\n",
      "Q:  When did you marry her ?\n",
      "\n",
      "S:  Vitamin C is found in colorful vegetables, for instance, bell peppers have a lot of vitamin C.\n",
      "John is feeling much better now.\n",
      "Q:  Give an instance where vitamin C is found in colorful vegetables, ?\n",
      "\n",
      "S:  I had been playing the drums since school time.\n",
      "Q:  When were you been playing the drums ?\n",
      "\n",
      "S:  I have been up since four.\n",
      "Q:  When do you hav been up ?\n",
      "\n",
      "S:  Population refers to the number of individuals in a particular place.\n",
      "Q:  Does population refers to the number of individuals in a particular place?\n",
      "\n",
      "S:  It is ten o’clock.\n",
      "Q:  Is it ten o ’ clock ?\n",
      "\n",
      "S:  It is ten o’clock.\n",
      "Q:  Where is It ten o ’ clock ?\n",
      "\n",
      "S:  Darjeeling is known for its beautiful tea gardens.\n",
      "Q:  Is darjeeling known for its beautiful tea gardens ?\n",
      "\n",
      "S:  We were playing tennis at the club.\n",
      "Q:  Were you playing tennis at the club ?\n",
      "\n",
      "S:  Mahatma Gandhi was born on 2nd October 1869 in Porbandar, Gujarat.\n",
      "Q:  Was mahatma Gandhi born on 2nd October 1869 in Porbandar , Gujarat ?\n",
      "\n",
      "S:  Mahatma Gandhi was born on 2nd October 1869 in Porbandar, Gujarat.\n",
      "Q:  Who was born on 2nd October 1869 in Porbandar, Gujarat?\n",
      "\n",
      "S:  Mahatma Gandhi was born on 2nd October 1869 in Porbandar, Gujarat.\n",
      "Q:  When was Mahatma Gandhi bear?\n",
      "\n",
      "S:  Mahatma Gandhi was born on 2nd October 1869 in Porbandar, Gujarat.\n",
      "Q:  Where was Mahatma Gandhi born on?\n",
      "\n",
      "S:  Mahatma Gandhi was born on 2nd October 1869 in Porbandar, Gujarat.\n",
      "Q:  Where was Mahatma Gandhi born on?\n",
      "\n",
      "S:  Abdul Kalam was an aerospace scientist who served as the 11th President of India from 2002 to 2007.\n",
      "Q:  Was abdul Kalam an aerospace scientist who served as the 11th President of India from 2002 to 2007 ?\n",
      "\n",
      "S:  Abdul Kalam was an aerospace scientist who served as the 11th President of India from 2002 to 2007.\n",
      "Q:  Who was an aerospace scientist who served as the 11th President of India from 2002 to 2007?\n",
      "\n",
      "S:  Abdul Kalam was an aerospace scientist who served as the 11th President of India from 2002 to 2007.\n",
      "Q:  Where was Abdul Kalam an aerospace scientist who served as?\n",
      "\n",
      "S:  Abdul Kalam was an aerospace scientist who served as the 11th President of India from 2002 to 2007.\n",
      "Q:  Where was Abdul Kalam an aerospace scientist who served as?\n",
      "\n",
      "S:  Abdul Kalam was an aerospace scientist who served as the 11th President of India from 2002 to 2007.\n",
      "Q:  When was Abdul Kalam an aerospace scientist who serve as the 11th President of India from?\n",
      "\n",
      "S:  Abdul Kalam was an aerospace scientist who served as the 11th President of India from 2002 to 2007.\n",
      "Q:  When was Abdul Kalam an aerospace scientist who serve as the 11th President of India from 2002 to?\n",
      "\n",
      "S:  Sachin Tendulkar was awarded Bharat Ratna in 2013.\n",
      "Q:  Was sachin Tendulkar awarded Bharat Ratna in 2013 ?\n",
      "\n",
      "S:  Sachin Tendulkar was awarded Bharat Ratna in 2013.\n",
      "Q:  Who was awarded Bharat Ratna in 2013?\n",
      "\n",
      "S:  Sachin Tendulkar was awarded Bharat Ratna in 2013.\n",
      "Q:  Who was Sachin Tendulkar awarded?\n",
      "\n",
      "S:  Sachin Tendulkar was awarded Bharat Ratna in 2013.\n",
      "Q:  When was Sachin Tendulkar award Bharat Ratna?\n",
      "\n",
      "S:  Delhi is the capital of India.\n",
      "Q:  Is delhi the capital of India ?\n",
      "\n",
      "S:  Delhi is the capital of India.\n",
      "Q:  Who is the capital of India?\n",
      "\n",
      "S:  Delhi is the capital of India.\n",
      "Q:  Where is Delhi the capital of India ?\n",
      "\n",
      "S:  Jawaharlal Nehru was born on 14th November 1889 in Allahabad, Uttar Pradesh.\n",
      "Q:  Was jawaharlal Nehru born on 14th November 1889 in Allahabad , Uttar Pradesh ?\n",
      "\n",
      "S:  Jawaharlal Nehru was born on 14th November 1889 in Allahabad, Uttar Pradesh.\n",
      "Q:  Who was born on 14th November 1889 in Allahabad, Uttar Pradesh?\n",
      "\n",
      "S:  Jawaharlal Nehru was born on 14th November 1889 in Allahabad, Uttar Pradesh.\n",
      "Q:  When was Jawaharlal Nehru bear?\n",
      "\n",
      "S:  Jawaharlal Nehru was born on 14th November 1889 in Allahabad, Uttar Pradesh.\n",
      "Q:  Where was Jawaharlal Nehru born on?\n",
      "\n",
      "S:  Jawaharlal Nehru was born on 14th November 1889 in Allahabad, Uttar Pradesh.\n",
      "Q:  Where was Jawaharlal Nehru born on?\n",
      "\n",
      "S:  I shall have been living in Mumbai for five years by May 2019.\n",
      "Q:  Were you have been living in Mumbai for five years by May 2019 ?\n",
      "\n",
      "S:  I shall have been living in Mumbai for five years by May 2019.\n",
      "Q:  Where shall I have been?\n",
      "\n",
      "S:  I shall have been living in Mumbai for five years by May 2019.\n",
      "Q:  When shall I have be live in Mumbai for?\n",
      "\n",
      "S:  I shall have been living in Mumbai for five years by May 2019.\n",
      "Q:  When shall I have be live in Mumbai for five years by?\n",
      "\n",
      "S:  She is preparing chicken sandwiches for breakfast.\n",
      "Q:  Is she preparing chicken sandwiches for breakfast ?\n",
      "\n",
      "S:  Yes, she is working very hard.\n",
      "Q:  Is she working very hard ?\n",
      "\n",
      "S:  I did go for fishing today.\n",
      "Q:  Were you go for fishing today ?\n",
      "\n",
      "S:  I did go for fishing today.\n",
      "Q:  When did I go for fishing?\n",
      "\n",
      "S:  He was elected as the Prime Minister of India on 15th August 1947.\n",
      "Q:  Was he elected as the Prime Minister of India on 15th August 1947 ?\n",
      "\n",
      "S:  He was elected as the Prime Minister of India on 15th August 1947.\n",
      "Q:  Where was He elected as?\n",
      "\n",
      "S:  He was elected as the Prime Minister of India on 15th August 1947.\n",
      "Q:  When was He elect as the Prime Minister of India?\n",
      "\n",
      "S:  I was playing tennis.\n",
      "Q:  Were you playing tennis ?\n",
      "\n",
      "S:  Hindi Diwas was first celebrated in the year 1953.\n",
      "Q:  Was hindi Diwas first celebrated in the year 1953 ?\n",
      "\n",
      "S:  Hindi Diwas was first celebrated in the year 1953.\n",
      "Q:  Who was first celebrated in the year 1953?\n",
      "\n",
      "S:  Hindi Diwas was first celebrated in the year 1953.\n",
      "Q:  When was Hindi Diwas first celebrate?\n",
      "\n",
      "S:  He will go to China tomorrow.\n",
      "Q:  Will he go to China tomorrow ?\n",
      "\n",
      "S:  He will go to China tomorrow.\n",
      "Q:  Where will He go to?\n",
      "\n",
      "S:  He will go to China tomorrow.\n",
      "Q:  When will He go to China?\n",
      "\n",
      "S:  The British had introduced a National Flag for British India after the revolt of 1857.\n",
      "Q:  Had the British introduced a National Flag for British India after the revolt of 1857 ?\n",
      "\n",
      "S:  The British had introduced a National Flag for British India after the revolt of 1857.\n",
      "Q:  Where had The British introduced a?\n",
      "\n",
      "S:  The British had introduced a National Flag for British India after the revolt of 1857.\n",
      "Q:  Where had The British introduced a?\n",
      "\n",
      "S:  The British had introduced a National Flag for British India after the revolt of 1857.\n",
      "Q:  Where had The British introduced a?\n",
      "\n",
      "S:  The British had introduced a National Flag for British India after the revolt of 1857.\n",
      "Q:  When had The British introduce a National Flag for British India after the revolt of?\n",
      "\n",
      "S:  The Taj Mahal is a beautiful monument built in 1631 by an Emperor named Shah Jahan in memory of his wife Mumtaz Mahal.\n",
      "Q:  Is the Taj Mahal a beautiful monument built in 1631 by an Emperor named Shah Jahan in memory of his wife Mumtaz Mahal ?\n",
      "\n",
      "S:  The Taj Mahal is a beautiful monument built in 1631 by an Emperor named Shah Jahan in memory of his wife Mumtaz Mahal.\n",
      "Q:  When is The Taj Mahal a beautiful monument build?\n",
      "\n",
      "S:  The Taj Mahal is a beautiful monument built in 1631 by an Emperor named Shah Jahan in memory of his wife Mumtaz Mahal.\n",
      "Q:  Who is The Taj Mahal a beautiful monument built in?\n",
      "\n",
      "S:  The Taj Mahal is a beautiful monument built in 1631 by an Emperor named Shah Jahan in memory of his wife Mumtaz Mahal.\n",
      "Q:  Where is The Taj Mahal a beautiful monument built in?\n",
      "\n",
      "S:  They have been trying to contact her.\n",
      "Q:  Do they hav been try to contact her?\n",
      "\n",
      "S:  They grow really well in pots.\n",
      "Q:  Do they grow really well in pots?\n",
      "\n",
      "S:  No, I was not playing cricket.\n",
      "Q:  Were you not playing cricket ?\n",
      "\n",
      "S:  His name is Peter.\n",
      "Q:  Is his name Peter ?\n",
      "\n",
      "S:  His name is Peter.\n",
      "Q:  Who is His name Peter ?\n",
      "\n",
      "S:  You usually walk to work.\n",
      "Q:  Do you usually walk to work?\n",
      "\n",
      "S:  Sun is the largest member of the Solar System.\n",
      "Q:  Is sun the largest member of the Solar System ?\n",
      "\n",
      "S:  Sun is the largest member of the Solar System.\n",
      "Q:  Who is the largest member of the Solar System?\n",
      "\n",
      "S:  Sun is the largest member of the Solar System.\n",
      "Q:  Where is Sun the largest member of the Solar System ?\n",
      "\n",
      "S:  John was held captive at Castle Black.\n",
      "Q:  Was john held captive at Castle Black ?\n",
      "\n",
      "S:  John was held captive at Castle Black.\n",
      "Q:  Who was held captive at Castle Black?\n",
      "\n",
      "S:  John was held captive at Castle Black.\n",
      "Q:  Where was John held captive?\n",
      "\n",
      "S:  Yes, I like coffee.\n",
      "Q:  Do you lik coffee?\n",
      "\n",
      "S:  Gandhi Jayanti is celebrated on 2nd October.\n",
      "Q:  Is gandhi Jayanti celebrated on 2nd October ?\n",
      "\n",
      "S:  Gandhi Jayanti is celebrated on 2nd October.\n",
      "Q:  Who is celebrated on 2nd October?\n",
      "\n",
      "S:  Gandhi Jayanti is celebrated on 2nd October.\n",
      "Q:  When is Gandhi Jayanti celebrate?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentensify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
