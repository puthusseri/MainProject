{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate MCQS from a given paragraph\n",
    "\n",
    "## TODO's\n",
    "\n",
    "- Generate Question\n",
    "- Answer that question\n",
    "- Generate Distractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 : Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yes_or_no_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class initializations\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold all input sentences\n",
    "sentences = []\n",
    "\n",
    "# Dictionary to hold sentences corresponding to respective discourse markers\n",
    "disc_sentences = {}\n",
    "\n",
    "# Remaining sentences which do not have discourse markers (To be used later to generate other kinds of questions)\n",
    "nondisc_sentences = []\n",
    "\n",
    "questions_list = []\n",
    "questions_answers_list = []\n",
    "yes_or_no_questions = []\n",
    "# List of auxiliary verbs\n",
    "aux_list = ['am', 'are', 'is', 'was', 'were', 'can', 'could', 'does', 'do', 'did', 'has', 'had', 'may', 'might', 'must',\n",
    "            'need', 'ought', 'shall', 'should', 'will', 'would']\n",
    "\n",
    "# List of all discourse markers\n",
    "discourse_markers = ['because', 'as a result', 'since', 'when', 'although', 'for example', 'for instance']\n",
    "\n",
    "# Different question types possible for each discourse marker\n",
    "qtype = {'because': ['Why'], 'since': ['When', 'Why'], 'when': ['When'], 'although': ['Yes/No'], 'as a result': ['Why'], \n",
    "        'for example': ['Give an example where'], 'for instance': ['Give an instance where'], 'to': ['Why']}\n",
    "\n",
    "# The argument which forms a question\n",
    "target_arg = {'because': 1, 'since': 1, 'when': 1, 'although': 1, 'as a result': 2, 'for example': 1, 'for instance': 1, \n",
    "              'to': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the paragraph into sentence\n",
    "def sentensify():\n",
    "    global sentences\n",
    "    global questions_list\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    #it contains the paragraph\n",
    "    \n",
    "    fp = open('input.txt')\n",
    "    data = fp.read()\n",
    "    sentences = tokenizer.tokenize(data)\n",
    "    question_answer = discourse()\n",
    "    for i in range(len(question_answer)):\n",
    "        questions_list.append(question_answer[i][1])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to generate the questions from sentences which have already been pre-processed.\n",
    "def generate_question(question_part, type):\n",
    "\n",
    "    ''' \n",
    "        question_part -> Part of input sentence which forms a question\n",
    "        type-> The type of question (why, where, etc)\n",
    "    '''\n",
    "    # Remove full stop and make first letter lower case\n",
    "    question_part = question_part[0].lower() + question_part[1:]\n",
    "    if(question_part[-1] == '.' or question_part[-1] == ','):\n",
    "        question_part = question_part[:-1]\n",
    "        \n",
    "    # Capitalizing 'i' since 'I' is recognized by parsers appropriately    \n",
    "    for i in range(0, len(question_part)):\n",
    "        if(question_part[i] == 'i'):\n",
    "            if((i == 0 and question_part[i+1] == ' ') or (question_part[i-1] == ' ' and question_part[i+1] == ' ')):\n",
    "                question_part = question_part[:i] + 'I' + question_part[i + 1: ]\n",
    "                \n",
    "    question = \"\"\n",
    "    if(type == 'Give an example where' or type == 'Give an instance where'):\n",
    "        question = type + \" \" + question_part + '?'\n",
    "        return question\n",
    "\n",
    "    aux_verb = False\n",
    "    res = None\n",
    "    \n",
    "    # Find out if auxiliary verb already exists\n",
    "    for i in range(len(aux_list)):\n",
    "        if(aux_list[i] in question_part.split()):\n",
    "            aux_verb = True\n",
    "            pos = i\n",
    "            break\n",
    "\n",
    "    # If auxiliary verb exists\n",
    "    if(aux_verb):\n",
    "        \n",
    "        # Tokeninze the part of the sentence from which the question has to be made\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        question_part = \"\"\n",
    "        fP = False\n",
    "        \n",
    "        for word, tag in tags:\n",
    "            if(word in ['I', 'We', 'we']):\n",
    "                question_part += 'you' + \" \"\n",
    "                fP = True\n",
    "                continue\n",
    "            question_part += word + \" \"\n",
    "\n",
    "        # Split across the auxiliary verb and prepend it at the start of question part\n",
    "        question = question_part.split(\" \" + aux_list[pos])\n",
    "        if(fP):\n",
    "             question = [\"were \"] + question\n",
    "        else:\n",
    "            question = [aux_list[pos] + \" \"] + question\n",
    "\n",
    "        # If Yes/No, no need to introduce question phrase\n",
    "        if(type == 'Yes/No'):\n",
    "            question += ['?']\n",
    "            \n",
    "        elif(type != \"non_disc\"):\n",
    "            question = [type + \" \"] + question + [\"?\"]\n",
    "            \n",
    "        else:\n",
    "            question = question + [\"?\"]\n",
    "         \n",
    "        question = ''.join(question)\n",
    "\n",
    "    # If auxilary verb does ot exist, it can only be some form of verb 'do'\n",
    "    else:\n",
    "        aux = None\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        comb = \"\"\n",
    "\n",
    "        '''There can be following combinations of nouns and verbs:\n",
    "            NN/NNP and VBZ  -> Does\n",
    "            NNS/NNPS(plural) and VBP -> Do\n",
    "            NN/NNP and VBN -> Did\n",
    "            NNS/NNPS(plural) and VBN -> Did\n",
    "        '''\n",
    "        \n",
    "        for tag in tags:\n",
    "            if(comb == \"\"):\n",
    "                if(tag[1] == 'NN' or tag[1] == 'NNP'):\n",
    "                    comb = 'NN'\n",
    "\n",
    "                elif(tag[1] == 'NNS' or tag[1] == 'NNPS'):\n",
    "                    comb = 'NNS'\n",
    "\n",
    "                elif(tag[1] == 'PRP'):\n",
    "                    if tag[0] in ['He','She','It']:\n",
    "                        comb = 'PRPS'\n",
    "                    else:\n",
    "                        comb = 'PRPP'\n",
    "                        tmp = question_part.split(\" \")\n",
    "                        tmp = tmp[1: ]\n",
    "                        if(tag[0] in ['I', 'we', 'We']):\n",
    "                            question_part = 'you ' + ' '.join(tmp)\n",
    "                            \n",
    "            if(res == None):\n",
    "                res = re.match(r\"VB*\", tag[1])\n",
    "                if(res):\n",
    "                    \n",
    "                    # Stem the verb\n",
    "                    question_part = question_part.replace(tag[0], stemmer.stem(tag[0]))\n",
    "                res = re.match(r\"VBN\", tag[1])\n",
    "                res = re.match(r\"VBD\", tag[1])\n",
    "\n",
    "        if(comb == 'NN'):\n",
    "            aux = 'does'\n",
    "            \n",
    "        elif(comb == 'NNS'):\n",
    "            aux = 'do'\n",
    "            \n",
    "        elif(comb == 'PRPS'):\n",
    "            aux = 'does'\n",
    "            \n",
    "        elif(comb == 'PRPP'):\n",
    "            aux = 'do'\n",
    "            \n",
    "        if(res and res.group() in ['VBD', 'VBN']):\n",
    "            aux = 'did'\n",
    "\n",
    "        if(aux):\n",
    "            if(type == \"non_disc\" or type == \"Yes/No\"):\n",
    "                question = aux + \" \" + question_part + \"?\"\n",
    "\n",
    "            else:\n",
    "                question = type + \" \" + aux + \" \" + question_part + \"?\"\n",
    "    if(question != \"\"):\n",
    "        question = question[0].upper() + question[1:]\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to get the named entities\n",
    "def get_named_entities(sent):\n",
    "    doc = nlp(sent)\n",
    "    named_entities = [(X.text, X.label_) for X in doc.ents]\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to get the required wh word\n",
    "def get_wh_word(entity, sent):\n",
    "    wh_word = \"\"\n",
    "    if entity[1] in ['TIME', 'DATE']:\n",
    "        wh_word = 'When'\n",
    "        \n",
    "    elif entity[1] == ['PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE']:\n",
    "        wh_word = 'What'\n",
    "        \n",
    "    elif entity[1] in ['PERSON']:\n",
    "            wh_word = 'Who'\n",
    "            \n",
    "    elif entity[1] in ['NORP', 'FAC' ,'ORG', 'GPE', 'LOC']:\n",
    "        index = sent.find(entity[0])\n",
    "        if index == 0:\n",
    "            wh_word = \"Who\"\n",
    "            \n",
    "        else:\n",
    "            wh_word = \"Where\"\n",
    "            \n",
    "    else:\n",
    "        wh_word = \"Where\"\n",
    "    return wh_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generate questions based on NER templates\n",
    "def generate_one_word_questions(sent):\n",
    "    \n",
    "    named_entities = get_named_entities(sent)\n",
    "    questions = []\n",
    "    \n",
    "    if not named_entities:\n",
    "        return questions\n",
    "    \n",
    "    for entity in named_entities:\n",
    "        wh_word = get_wh_word(entity, sent)\n",
    "        \n",
    "        if(sent[-1] == '.'):\n",
    "            sent = sent[:-1]\n",
    "        \n",
    "        if sent.find(entity[0]) == 0:\n",
    "            questions.append(sent.replace(entity[0],wh_word) + '?')\n",
    "            continue\n",
    "       \n",
    "        question = \"\"\n",
    "        aux_verb = False\n",
    "        res = None\n",
    "\n",
    "        for i in range(len(aux_list)):\n",
    "            if(aux_list[i] in sent.split()):\n",
    "                aux_verb = True\n",
    "                pos = i\n",
    "                break\n",
    "            \n",
    "        if not aux_verb:\n",
    "            pos = 9\n",
    "        \n",
    "        text = nltk.word_tokenize(sent)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        question_part = \"\"\n",
    "        \n",
    "        if wh_word == 'When':\n",
    "            word_list = sent.split(entity[0])[0].split()\n",
    "            if word_list[-1] in ['in', 'at', 'on']:\n",
    "                question_part = \" \".join(word_list[:-1])\n",
    "            else:\n",
    "                question_part = \" \".join(word_list)\n",
    "            \n",
    "            qp_text = nltk.word_tokenize(question_part)\n",
    "            qp_tags = nltk.pos_tag(qp_text)\n",
    "            \n",
    "            question_part = \"\"\n",
    "            \n",
    "            for i, grp in enumerate(qp_tags):\n",
    "                word = grp[0]\n",
    "                tag = grp[1]\n",
    "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
    "                    question_part += WordNetLemmatizer().lemmatize(word,'v') + \" \"\n",
    "                else:\n",
    "                    question_part += word + \" \"\n",
    "                \n",
    "            if question_part[-1] == ' ':\n",
    "                question_part = question_part[:-1]\n",
    "        \n",
    "        else:\n",
    "            for i, grp in enumerate(tags):\n",
    "                \n",
    "                #Break the sentence after the first non-auxiliary verb\n",
    "                word = grp[0]\n",
    "                tag = grp[1]\n",
    "\n",
    "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
    "                    question_part += word\n",
    "\n",
    "                    if i<len(tags) and 'NN' not in tags[i+1][1] and wh_word != 'When':\n",
    "                        question_part += \" \"+ tags[i+1][0]\n",
    "\n",
    "                    break\n",
    "                question_part += word + \" \"\n",
    "        question = question_part.split(\" \"+ aux_list[pos])\n",
    "        question = [aux_list[pos] + \" \"] + question\n",
    "        question = [wh_word+ \" \"] + question + [\"?\"]\n",
    "        question = ''.join(question)\n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the sentence and find if any discourses are there in it. so that questions can be made from it\n",
    "def discourse():\n",
    "    temp = []\n",
    "    target = \"\"\n",
    "    questions = []\n",
    "    global disc_sentences\n",
    "    global yes_or_no_questions\n",
    "    yes_or_no_questions = []\n",
    "    disc_sentences = {}\n",
    "    for i in range(len(sentences)):\n",
    "        maxLen = 9999999\n",
    "        val = -1\n",
    "        for j in discourse_markers:\n",
    "            tmp = len(sentences[i].split(j)[0].split(' '))  \n",
    "            \n",
    "            # To get valid, first discourse marker.   \n",
    "            if(len(sentences[i].split(j)) > 1 and tmp >= 3 and tmp < maxLen):\n",
    "                maxLen = tmp\n",
    "                val = j\n",
    "                \n",
    "        if(val != -1):\n",
    "\n",
    "            # To initialize a list for every new key\n",
    "            if(disc_sentences.get(val, 'empty') == 'empty'):\n",
    "                disc_sentences[val] = []\n",
    "                \n",
    "            disc_sentences[val].append(sentences[i])\n",
    "            temp.append(sentences[i])\n",
    "\n",
    "\n",
    "    nondisc_sentences = list(set(sentences) - set(temp))\n",
    "    \n",
    "    t = []\n",
    "    for k, v in disc_sentences.items():\n",
    "        for val in range(len(v)):\n",
    "            \n",
    "            # Split the sentence on discourse marker and identify the question part\n",
    "            question_part = disc_sentences[k][val].split(k)[target_arg[k] - 1]\n",
    "            q = generate_question(question_part, qtype[k][0])\n",
    "            if(q != \"\"):\n",
    "                if qtype[k][0] == \"Yes/No\":\n",
    "                    yes_or_no_questions.append(q)\n",
    "                else:\n",
    "                    questions.append([disc_sentences[k][val],q])\n",
    "                \n",
    "                \n",
    "    for question_part in nondisc_sentences:\n",
    "        s = \"non_disc\"\n",
    "        sentence = question_part\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        if(text[0] == 'Yes'):\n",
    "            question_part = question_part[5:]\n",
    "            s = \"Yes/No\"\n",
    "            \n",
    "        elif(text[0] == 'No'):\n",
    "            question_part = question_part[4:]\n",
    "            s = \"Yes/No\"\n",
    "            \n",
    "        q = generate_question(question_part, s)\n",
    "        if(q != \"\"):\n",
    "            if s==\"Yes/No\":\n",
    "                yes_or_no_questions_or_no_questions.append(q)\n",
    "            else:\n",
    "                questions.append([sentence,q])\n",
    "                \n",
    "        l = generate_one_word_questions(question_part)\n",
    "        questions += [[sentence,i] for i in l]\n",
    "    print(len(questions))\n",
    "    return questions        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "sentensify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 : Answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if using for the first time\n",
    "\n",
    "# !pip install -U spacy\n",
    "# !pip install -U spacy-lookups-data\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !conda install numpy\n",
    "# !conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "#!pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyshak/anaconda3/envs/tensornlp/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/vyshak/anaconda3/envs/tensornlp/lib/python3.7/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:56: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passage = \"\"\"COVID-19 is the infectious disease caused by the most recently discovered coronavirus. This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019. The most common symptoms of COVID-19 are fever, tiredness, and dry cough. Some patients may have aches and pains, nasal congestion, runny nose, sore throat or diarrhea. These symptoms are usually mild and begin gradually. Some people become infected but donâ€™t develop any symptoms and don't feel unwell. Most people (about 80%) recover from the disease without needing special treatment. Around 1 out of every 6 people who gets COVID-19 becomes seriously ill and develops difficulty breathing. Older people, and those with underlying medical problems like high blood pressure, heart problems or diabetes, are more likely to develop serious illness. People with fever, cough and difficulty breathing should seek medical attention.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global questions_answers_list\n",
    "questions_answers_list = []\n",
    "f = open(\"input.txt\")\n",
    "data = f.read()\n",
    "for i in questions_list:\n",
    "    result = predictor.predict(passage=data,question=i)\n",
    "    questions_answers_list.append([i,result['best_span_str']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(passage=data,question=questions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frequent and destructive eruptions'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['best_span_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why has mount Vesuvius , a volcano located between the ancient Italian cities of Pompeii and Herculaneum , received much attention ?',\n",
       " 'frequent and destructive eruptions']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_answers_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 : Distractor generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential,Model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>distractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meals can be served</td>\n",
       "      <td>in rooms at 9:00 p. m.</td>\n",
       "      <td>'outside the room at 3:00 p. m.', 'in the dini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It can be inferred from the passage that</td>\n",
       "      <td>The local government can deal with the problem...</td>\n",
       "      <td>'If some tragedies occur again ', ' relevant d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The author called Tommy 's parents in order to</td>\n",
       "      <td>help them realize their influence on Tommy</td>\n",
       "      <td>'blame Tommy for his failing grades', 'blame T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It can be inferred from the passage that</td>\n",
       "      <td>the writer is not very willing to use idioms</td>\n",
       "      <td>'idioms are the most important part in a langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can we deal with snake wounds according to...</td>\n",
       "      <td>Stay calm and do n't move .</td>\n",
       "      <td>'Cut the wound and suck the poison out .'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                                Meals can be served   \n",
       "1           It can be inferred from the passage that   \n",
       "2     The author called Tommy 's parents in order to   \n",
       "3           It can be inferred from the passage that   \n",
       "4  How can we deal with snake wounds according to...   \n",
       "\n",
       "                                         answer_text  \\\n",
       "0                             in rooms at 9:00 p. m.   \n",
       "1  The local government can deal with the problem...   \n",
       "2         help them realize their influence on Tommy   \n",
       "3       the writer is not very willing to use idioms   \n",
       "4                        Stay calm and do n't move .   \n",
       "\n",
       "                                          distractor  \n",
       "0  'outside the room at 3:00 p. m.', 'in the dini...  \n",
       "1  'If some tragedies occur again ', ' relevant d...  \n",
       "2  'blame Tommy for his failing grades', 'blame T...  \n",
       "3  'idioms are the most important part in a langu...  \n",
       "4          'Cut the wound and suck the poison out .'  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"Train.csv\")\n",
    "test = pd.read_csv(\"Test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.values\n",
    "train = train.values\n",
    "answers = {}\n",
    "distractors = {}\n",
    "count = 0\n",
    "for x in range(train.shape[0]):\n",
    "    answers[train[x][0]] = train[x][1]\n",
    "    a=[]\n",
    "    for y in train[x][2].split(\", \"):\n",
    "        a.append(str(y[1:-1]))\n",
    "    distractors[train[x][0]] = a\n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outside the room at 3:00 p. m.',\n",
       " 'in the dining - room at 6:00 p. m.',\n",
       " 'in the dining - room from 7:30 a. m. to 9:15 p. m.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distractors[\"Meals can be served\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\"[^a-z0-9]+\",\" \" , sentence)\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    sentence = [s for s in sentence if((len(s)>1) or (re.match(\"[0-9]+\",s) is not None))]\n",
    "    sentence = \" \".join(sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all the captions\n",
    "a={}\n",
    "d={}\n",
    "for key , dist_list in distractors.items():\n",
    "    for i in range(len(dist_list)):\n",
    "        dist_list[i] = clean_text(dist_list[i])\n",
    "    answer=clean_text(answers[key])\n",
    "    key=clean_text(key)\n",
    "    a[key]=answer\n",
    "    d[key]=dist_list\n",
    "answers=a\n",
    "distractors=d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outside the room at 3 00',\n",
       " 'in the dining room at 6 00',\n",
       " 'in the dining room from 7 30 to 9 15']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distractors[\"meals can be served\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answers.txt\",\"w\") as f:\n",
    "    f.write(str(answers))\n",
    "with open(\"distractors.txt\",\"w\") as f:\n",
    "    f.write(str(distractors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21459\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for key in answers.keys():\n",
    "    [vocab.update(key.split())]\n",
    "    [vocab.update(answers[key].split())]\n",
    "    [vocab.update(sentence.split()) for sentence in distractors[key]]\n",
    "\n",
    "    \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718584\n"
     ]
    }
   ],
   "source": [
    "total = []\n",
    "for key in answers.keys():\n",
    "    [total.append(i) for i in key.split()]\n",
    "    [total.append(i) for i in answers[key].split()]\n",
    "    [total.append(i) for des in distractors[key] for i in des.split()]\n",
    "\n",
    "print(len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4723\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "counter = collections.Counter(total)\n",
    "freq_cnt = dict(counter)\n",
    "\n",
    "sorted_freq_cnt = sorted(freq_cnt.items(),reverse = True,key = lambda x:x[1])\n",
    "\n",
    "threshold =10\n",
    "sorted_freq_cnt = [x for x in sorted_freq_cnt if x[1]>threshold]\n",
    "total_words = [x[0] for x in sorted_freq_cnt]\n",
    "print(len(sorted_freq_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['StartSeq outside the room at 3 00 EndSeq', 'StartSeq in the dining room at 6 00 EndSeq', 'StartSeq in the dining room from 7 30 to 9 15 EndSeq']\n"
     ]
    }
   ],
   "source": [
    "train_distractors = {}\n",
    "for key in distractors.keys():\n",
    "    train_distractors[key] = []\n",
    "    for dist in distractors[key]:\n",
    "        dist_to_append = \"StartSeq \" + dist + \" EndSeq\"\n",
    "        train_distractors[key].append(dist_to_append)\n",
    "        \n",
    "print(train_distractors[\"meals can be served\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4723"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4723"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "\n",
    "for i,word in enumerate(total_words):\n",
    "    word_to_idx[word] = i+1\n",
    "    idx_to_word[i+1] = word\n",
    "    \n",
    "len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx[\"StartSeq\"]=4724\n",
    "idx_to_word[4724] = \"StartSeq\"\n",
    "\n",
    "word_to_idx[\"EndSeq\"]=4725\n",
    "idx_to_word[4725] = \"EndSeq\"\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "vocab_size= vocab_size+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4726"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "max_len=0\n",
    "for key in train_distractors.keys():\n",
    "    for dist in train_distractors[key]:\n",
    "        max_len = max(max_len,len(dist.split()))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "max_q=0\n",
    "for key in train_distractors.keys():\n",
    "    max_q = max(max_q,len(key.split()))\n",
    "print(max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "max_a = 0\n",
    "for key in answers.keys():\n",
    "    max_a = max(max_a,len(answers[key].split()))\n",
    "print(max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_distractors,answers,word_to_idx,max_len,batch_size):\n",
    "    X1,X2,X3,y = [],[],[],[]\n",
    "\n",
    "    n=0\n",
    "    while True:\n",
    "        for key,dist_list in train_distractors.items():\n",
    "            n+=1\n",
    "\n",
    "            question = key\n",
    "            answer = answers[key]\n",
    "\n",
    "\n",
    "            seqq = [word_to_idx[wordQ] for wordQ in question.split() if wordQ in word_to_idx]\n",
    "            question= pad_sequences([seqq],maxlen=max_q,value=0,padding='post')[0]\n",
    "\n",
    "\n",
    "            seqa = [word_to_idx[wordA] for wordA in answer.split() if wordA in word_to_idx]\n",
    "            answer = pad_sequences([seqa],maxlen=max_a,value=0,padding='post')[0]\n",
    "\n",
    "            for dist in dist_list:\n",
    "                seq = [word_to_idx[word] for word in dist.split() if word in word_to_idx]\n",
    "                for i in range(1,len(seq)):\n",
    "                    xi = seq[0:i]\n",
    "                    yi = seq[i]\n",
    "\n",
    "                    xi = pad_sequences([xi],maxlen=max_len,value = 0,padding='post')[0] \n",
    "                    yi = to_categorical([yi],num_classes = vocab_size)[0]\n",
    "\n",
    "\n",
    "\n",
    "                    X1.append(question)\n",
    "                    X2.append(answer)\n",
    "                    X3.append(xi)\n",
    "                    y.append(yi)\n",
    "                if n==batch_size:\n",
    "                    yield[[np.array(X1),np.array(X2),np.array(X3)],np.array(y)]\n",
    "                    X1,X2,X3,y = [],[],[],[]\n",
    "                    n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"glove.6B.100d.txt\",encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "for line in f:\n",
    "  values=line.split()\n",
    "  word = values[0]\n",
    "  embedding_index[word]=np.array(values[1:],dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5985   , -0.46321  ,  0.13001  , -0.019576 ,  0.4603   ,\n",
       "       -0.3018   ,  0.8977   , -0.65634  ,  0.66858  , -0.49164  ,\n",
       "        0.037557 , -0.050889 ,  0.6451   , -0.53882  , -0.3765   ,\n",
       "       -0.04312  ,  0.51384  ,  0.17783  ,  0.28596  ,  0.92063  ,\n",
       "       -0.49349  , -0.48583  ,  0.61321  ,  0.78211  ,  0.19254  ,\n",
       "        0.91228  , -0.055596 , -0.12512  , -0.65688  ,  0.068557 ,\n",
       "        0.55629  ,  1.611    , -0.0073642, -0.48879  ,  0.45493  ,\n",
       "        0.96105  , -0.063369 ,  0.17432  ,  0.9814   , -1.3125   ,\n",
       "       -0.15801  , -0.54301  , -0.13888  , -0.26146  , -0.3691   ,\n",
       "        0.26844  , -0.24375  , -0.19484  ,  0.62583  , -0.7377   ,\n",
       "        0.38351  , -0.75004  , -0.39053  ,  0.091498 , -0.36591  ,\n",
       "       -1.4715   , -0.45228  ,  0.2256   ,  1.1412   , -0.38526  ,\n",
       "       -0.06716  ,  0.57288  , -0.39191  ,  0.31302  , -0.29235  ,\n",
       "       -0.96157  ,  0.15154  , -0.21659  ,  0.25103  ,  0.096967 ,\n",
       "        0.2843   ,  1.4296   , -0.50565  , -0.51374  , -0.47218  ,\n",
       "        0.32036  ,  0.023149 ,  0.22623  , -0.09725  ,  0.82126  ,\n",
       "        0.92599  , -1.0086   , -0.38639  ,  0.86408  , -1.206    ,\n",
       "       -0.28528  ,  0.2265   , -0.38773  ,  0.40879  ,  0.59303  ,\n",
       "        0.30769  ,  0.83804  , -0.63655  , -0.44639  , -0.43406  ,\n",
       "       -0.79364  , -0.28675  , -0.034398 ,  1.3431   ,  0.34904  ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix():\n",
    "    emb_dim=100\n",
    "    matrix = np.zeros((vocab_size,emb_dim))\n",
    "    for word,idx in word_to_idx.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            matrix[idx] = embedding_vector\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4726, 100)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = getEmbeddingMatrix()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[4724]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vyshak/anaconda3/envs/tensornlp/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/vyshak/anaconda3/envs/tensornlp/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "input_dist = Input(shape = (max_len,))\n",
    "input_dist1=  Embedding(input_dim=vocab_size,output_dim=100,mask_zero=True)(input_dist)\n",
    "input_dist2 = Dropout(0.3)(input_dist1)\n",
    "input_dist3 = LSTM(256)(input_dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ques = Input(shape = (max_q,))\n",
    "input_ques1=  Embedding(input_dim=vocab_size,output_dim=100,mask_zero=True)(input_ques)\n",
    "input_ques2 = Dropout(0.3)(input_ques1)\n",
    "input_ques3 = LSTM(256)(input_ques2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ans = Input(shape = (max_a,))\n",
    "input_ans1=  Embedding(input_dim=vocab_size,output_dim=100,mask_zero=True)(input_ans)\n",
    "input_ans2 = Dropout(0.3)(input_ans1)\n",
    "input_ans3 = LSTM(256)(input_ans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1 = add([input_dist3,input_ques3,input_ans3])\n",
    "decoder2 = Dense(512 ,activation = 'relu')(decoder1)\n",
    "outputs = Dense(vocab_size,activation= 'softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs = [input_ques,input_ans,input_dist],outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 48)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 30, 100)      472600      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 48, 100)      472600      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 101, 100)     472600      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 30, 100)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 48, 100)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 101, 100)     0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          365568      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          365568      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 256)          365568      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          131584      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4726)         2424438     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,070,526\n",
      "Trainable params: 5,070,526\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[3].set_weights([embedding_matrix])\n",
    "model.layers[3].trainable = False  \n",
    "\n",
    "model.layers[4].set_weights([embedding_matrix])\n",
    "model.layers[4].trainable = False  \n",
    "model.layers[5].set_weights([embedding_matrix])\n",
    "model.layers[5].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.load_weights(\"./model_weights/model_58.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=20\n",
    "# number_of_ques = 64\n",
    "# steps = len(train_distractors)//number_of_ques\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     generator = data_generator(train_distractors,answers,word_to_idx,max_len,number_of_ques)\n",
    "#     model.fit_generator(generator,epochs=1,steps_per_epoch = steps,verbose = 1)\n",
    "#     model.save(\"./model_weights/model_\"+str(i)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "        answers_t = {}\n",
    "        count = 0\n",
    "        for x in range(test.shape[0]):\n",
    "            answers_t[test[x][0]] = test[x][1]\n",
    "            count = count+1\n",
    "\n",
    "        a={}\n",
    "        for key , answer in answers_t.items():\n",
    "            answer=clean_text(answers_t[key])\n",
    "            key=clean_text(key)\n",
    "            a[key]=answer\n",
    "        answers_t=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lack of water affects california crops\n"
     ]
    }
   ],
   "source": [
    "print(answers_t[\"what the main idea of the text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10353\n"
     ]
    }
   ],
   "source": [
    "print(len(answers_t.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_distractors(X1,X2):\n",
    "    dists = []\n",
    "    for j in range(3):\n",
    "        in_text = \"StartSeq\"\n",
    "        for i in range(max_len):\n",
    "            sequence = [word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n",
    "            sequence = pad_sequences([sequence],maxlen=max_len,padding = 'post')[0]\n",
    "            XQ = []\n",
    "            XA = []\n",
    "            XI = []\n",
    "            XQ.append(X1)\n",
    "            XA.append(X2)\n",
    "            XI.append(sequence)\n",
    "            y_pred = model.predict([np.array(XQ),np.array(XA),np.array(XI)])\n",
    "\n",
    "            if(i<=1):\n",
    "                y_pred=np.array(y_pred)\n",
    "                y_pred = y_pred.argsort()\n",
    "                y_pred=y_pred[0][:]\n",
    "                y_pred=y_pred[len(y_pred)-1-j]\n",
    "            else:\n",
    "                y_pred=y_pred.argmax()\n",
    "            word = idx_to_word[y_pred]\n",
    "            in_text += (' ' + word)\n",
    "\n",
    "            if word == 'EndSeq':\n",
    "                break\n",
    "        final_dists = in_text.split()[1:-1]\n",
    "        final_dists = ' '.join(final_dists)\n",
    "        dists.append(final_dists)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MCQ(que,ans):\n",
    "    global questions_answers_distractors_list\n",
    "    question= clean_text(que)\n",
    "    answer = clean_text(ans)\n",
    "\n",
    "\n",
    "#     print(question)\n",
    "#     print(answer)\n",
    "    seqq = [word_to_idx[wordQ] for wordQ in question.split() if wordQ in word_to_idx]\n",
    "    question= pad_sequences([seqq],maxlen=max_q,value=0,padding='post')[0]\n",
    "\n",
    "    seqa = [word_to_idx[wordA] for wordA in answer.split() if wordA in word_to_idx]\n",
    "    answer = pad_sequences([seqa],maxlen=max_a,value=0,padding='post')[0]\n",
    "\n",
    "    #   question = question.reshape((1,question.shape[0]))\n",
    "    #   answer = answer.reshape((1,answer.shape[0]))\n",
    "\n",
    "\n",
    "    distractor = predict_distractors(question,answer)\n",
    "    distractor = str(distractor)\n",
    "    questions_answers_distractors_list.append([que,ans,distractor])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions_answers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yes_or_no_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vyshak/anaconda3/envs/tensornlp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(questions_answers_list)):\n",
    "    que = questions_answers_list[i][0]\n",
    "    ans = questions_answers_list[i][1]\n",
    "    generate_MCQ(que,ans)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why has mount Vesuvius , a volcano located between the ancient Italian cities of Pompeii and Herculaneum , received much attention ? \n",
      "\n",
      "frequent and destructive eruptions \n",
      "\n",
      "['and low city', 'ancient and', 'the railway building'] \n",
      "\n",
      "\n",
      "Was fire , however , not the only cause of destruction ? \n",
      "\n",
      "Sparks from the burning ash ignited the combustible rooftops quickly. Large portions of the city were destroyed in the conflagration. Fire, however, was not the only cause of destruction \n",
      "\n",
      "['the nail was out out alive', 'people burnt in the', 'farmers had been asked by the rich people'] \n",
      "\n",
      "\n",
      "Did by analys data, much as a zoologist dissect an animal specimen, scientists hav conclud that the eruption chang large portions of the areaâ€™s geography? \n",
      "\n",
      "large portions of the areaâ€™s geography \n",
      "\n",
      "['the of the united states', 'and and', 'people of the similar of the land'] \n",
      "\n",
      "\n",
      "Did sparks from the burn ash ignit the combustible rooftops quickly? \n",
      "\n",
      "Sparks from the burning ash ignited the combustible rooftops quickly \n",
      "\n",
      "['the worker to take the place', 'to jump down the water', 'using water to appear in the ground'] \n",
      "\n",
      "\n",
      "Do over the years, excavations of Pompeii and Herculaneum hav rev a great deal about the behavior of the volcano? \n",
      "\n",
      "a great deal \n",
      "\n",
      "['the growth rings', '', 'it was the world'] \n",
      "\n",
      "\n",
      "When did Over? \n",
      "\n",
      "A.D. 79 \n",
      "\n",
      "['the same time', 'two', 'computer companies'] \n",
      "\n",
      "\n",
      "Where did Over the years , excavations of Pompeii and Herculaneum have revealed? \n",
      "\n",
      "the volcano \n",
      "\n",
      "['the floor', '', 'allow flying'] \n",
      "\n",
      "\n",
      "Who did Over the years , excavations of Pompeii and Herculaneum have revealed? \n",
      "\n",
      "a great deal about the behavior of the volcano \n",
      "\n",
      "['the movement of the service team', 'might take the place of the plane', 'more than the people who lived in the area'] \n",
      "\n",
      "\n",
      "Did early the next morning, the volcano pour a huge river of molten rock down upon Herculaneum, completely burying the city and filling the harbor with coagulated lava? \n",
      "\n",
      "Mount Vesuvius, a volcano located between the ancient Italian cities of Pompeii and Herculaneum, has received much attention because of its frequent and destructive eruptions. The most famous of these eruptions occurred in A.D. 79.\n",
      "\n",
      "The volcano had been inactive for centuries. There was little warning of the coming eruption, although one account unearthed by archaeologists says that a hard rain and a strong wind had disturbed the celestial calm during the preceding night. Early the next morning, the volcano poured a huge river of molten rock down upon Herculaneum, completely burying the city and filling the harbor with coagulated lava \n",
      "\n",
      "['the city of the area of the area of the city of the city of the city', 'it was said the police police to take private to the tigers', 'there are tigers and there of the newly crowd'] \n",
      "\n",
      "\n",
      "Did poisonous sulfuric gases sat the air? \n",
      "\n",
      "Poisonous sulfuric gases saturated the air \n",
      "\n",
      "['the air of the earth', 'collecting the water supply', 'water air'] \n",
      "\n",
      "\n",
      "Can the eruption of Mount Vesuvius and its tragic consequences have provided everyone with a wealth of data about the effects that volcanoes have on the surrounding area ? \n",
      "\n",
      "can have on the surrounding area \n",
      "\n",
      "['can add the number of the number', 'will take the place of indoors', 'the may cause the'] \n",
      "\n",
      "\n",
      "Where can The eruption of Mount Vesuvius and its tragic consequences have provided? \n",
      "\n",
      "on the surrounding area \n",
      "\n",
      "['in the', 'between george', 'for new community'] \n",
      "\n",
      "\n",
      "Can today , volcanologists locate and predict eruptions , saving lives and preventing the destruction of other cities and cultures ? \n",
      "\n",
      "Today, volcanologists can locate and predict eruptions \n",
      "\n",
      "['people can enjoy happiness', 'the people who have poor health', 'it always have the same number of society'] \n",
      "\n",
      "\n",
      "When, volcanologists can locate and predict eruptions, saving lives and preventing the destruction of other cities and cultures? \n",
      "\n",
      "A.D. 79 \n",
      "\n",
      "['the system of the system', 'and around the world', ''] \n",
      "\n",
      "\n",
      "Do by strengthening the brittle bones with acrylic paint, scientists hav been able to examin the skeletons and draw conclusions about the diet and habits of the residents? \n",
      "\n",
      "draw conclusions \n",
      "\n",
      "['doing shopping', 'other foods', 'the french companies'] \n",
      "\n",
      "\n",
      "Did the most famous of these eruptions occur in A.D. 79? \n",
      "\n",
      "The most famous of these eruptions \n",
      "\n",
      "['the wonderful world', 'there are many people worldwide', 'they can get many presents'] \n",
      "\n",
      "\n",
      "When did The most famous of these eruptions occur? \n",
      "\n",
      "A.D. 79 \n",
      "\n",
      "['', 'and children', 'the same size of the house'] \n",
      "\n",
      "\n",
      "Had the volcano been inactive for centuries ? \n",
      "\n",
      "The volcano had been inactive for centuries \n",
      "\n",
      "['they knew how to build the', 'the was taken seriously by the explosion', 'there would be day day'] \n",
      "\n",
      "\n",
      "When had The volcano be inactive for? \n",
      "\n",
      "centuries \n",
      "\n",
      "['', 'the machine', 'people'] \n",
      "\n",
      "\n",
      "Is finally , the excavations at both Pompeii and Herculaneum have yielded many examples of classical art , such as jewelry made of bronze , which an alloy of copper and tin ? \n",
      "\n",
      "acrylic paint \n",
      "\n",
      "['white', 'dark red', 'culture coast'] \n",
      "\n",
      "\n",
      "Who is Finally , the excavations at both Pompeii and Herculaneum have yielded? \n",
      "\n",
      "many examples of classical art \n",
      "\n",
      "['little of', 'all of them hope to accept the idea', 'more famous students'] \n",
      "\n",
      "\n",
      "Who is Finally , the excavations at both Pompeii and Herculaneum have yielded? \n",
      "\n",
      "many examples of classical art \n",
      "\n",
      "['little of', 'all of them hope to accept the idea', 'more famous students'] \n",
      "\n",
      "\n",
      "Did for instance, it turn the Sarno River from its course and raised the level of the beach along the Bay of Naples? \n",
      "\n",
      "raised the level of the beach along the Bay of Naples \n",
      "\n",
      "['the city and the streets of hawaii', '000 of the city', 'and took the city to the united states'] \n",
      "\n",
      "\n",
      "Where did For instance , it turned the? \n",
      "\n",
      "Sarno River \n",
      "\n",
      "['the water', '', 'around and'] \n",
      "\n",
      "\n",
      "Where did For instance , it turned the? \n",
      "\n",
      "Sarno River \n",
      "\n",
      "['the water', '', 'around and'] \n",
      "\n",
      "\n",
      "Did meteorologists study these events hav also conclud that Vesuvius caus a huge tidal wave that affected the worldâ€™s climate? \n",
      "\n",
      "a huge tidal wave that affected the worldâ€™s climate \n",
      "\n",
      "['the majority of the technology balance system', 'people of the climate of the pollution of the rocks', 'there were many kinds of houses in the city of the united states'] \n",
      "\n",
      "\n",
      "Where did Meteorologists studying these? \n",
      "\n",
      "Bay of Naples \n",
      "\n",
      "['near the united states', 'the united museum of england', 'english teachers'] \n",
      "\n",
      "\n",
      "Does in addition to mak these investigations, archaeologists hav been able to study the skeletons of victims by us distilled water to wash away the volcanic ash? \n",
      "\n",
      "scientists have been able to examine the skeletons \n",
      "\n",
      "['the plants would be put into the market', 'scientists will take the place where to the', 'there will be no easy to reduce the crops'] \n",
      "\n",
      "\n",
      "Were large portions of the city destroyed in the conflagration ? \n",
      "\n",
      "Large portions of the city were destroyed in the conflagration \n",
      "\n",
      "['people in the city to city', 'and and gas city', 'the and police to city'] \n",
      "\n",
      "\n",
      "Did meanwhile, on the other side of the mountain, cinders, stone and ash rain down on Pompeii? \n",
      "\n",
      "lava \n",
      "\n",
      "['the water', 'other animals', 'elephant and grass'] \n",
      "\n",
      "\n",
      "Where did Meanwhile , on the other side of the mountain , cinders , stone and ash rained down? \n",
      "\n",
      "Pompeii \n",
      "\n",
      "['elephant', '', 'the house'] \n",
      "\n",
      "\n",
      "Were these heavy gases not buoyant in the atmosphere and therefore sank toward the earth and suffocated people ? \n",
      "\n",
      "Poisonous sulfuric gases saturated the air. These heavy gases were not buoyant in the atmosphere and therefore sank toward the earth and suffocated people \n",
      "\n",
      "['the environment where the city where the environment of radiation', 'what people throw the direction where the earth', 'people in throw the city of the earth'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in questions_answers_distractors_list:\n",
    "    \n",
    "    print(i[0],\"\\n\")\n",
    "    print(i[1],\"\\n\")\n",
    "    print(i[2],\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
